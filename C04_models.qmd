---
title: "Ensembles"
format: html
---

> All models are wrong, but some are useful.
>   ~ [George Box](https://en.wikipedia.org/wiki/George_E._P._Box)

Modeling starts with a collection of observations (presence and background for us!) and ends up with a collection of coeefficients that can be used with one or more formulas to make a predicition for the past, the present or the future.  We are using modeling specifically to make habitat suitability maps for select species under two climate scenarios (RCP45 and RCP85) at two different times (2055 and 2075) in the future.

We can choose from a number of different models: [random forest](https://en.wikipedia.org/wiki/Random_forest) (rf), [maximum entropy](https://en.wikipedia.org/wiki/Principle_of_maximum_entropy) (maxent or maxnet), [boosted regression trees](boosted regression trees) (brt), [general linear models](https://en.wikipedia.org/wiki/General_linear_model) (glm), etc.  The point of each is to make a mathematical representation of natural occurrences. It is important to consider what those occurences might be - categorical like labels? likelihoods like probabilities? continuous like measurements?  Here are examples of each...

+ **Categorical**
  - two class labels: "present/absence", "red/green", "shell/no shell", "alive/dead"
  - multi-class labels: "vanilla/chocolate/strawberry", "immature/mature/aged"
  
+ **Likelihood and Probability**
  - probability: "50% chance of rain", "80% chance of a fatal fall"
  - relativity: "low likelihood of encounter", "some likelihood of encounter"
  
+ **Continuous**
  - abundance: "48.2 mice per km^2", "10,500 copepods per m^3"
  - rate: "50 knot winds", "28.2 Svedrups"
  - measure: "3.2 cm of rain", "12.1 grams of carbon"
  
We are modeling with known observations (presences) and a sampling of the background, so we are trying to model a likelihood that a species will be encountered (and reported) relative to the environmental conditions.  So we are looking for a model that can produce relative likelihood of an encounter that results in a report.

We'll be using a random forest model (rf), and we'll be following this [tidy models tutorial](https://oj713.github.io/tidymodels/index.html) prepared by our colleague [Omi Johnson](https://omi-johnson.netlify.app/).

# Setup 

As always, we start by running our setup function. Start RStudio/R, and reload your project with the menu `File > Recent Projects`. 

```{r setup}
source("setup.R")
```

# Load data - choose a month and sampling approach

Let's load what we need to build a model for August using the greedy sampling technique. We'll also need the model configuration (which is "g008"). And we'll need the covariate data.

```{r load_data}
model_input = read_model_input(scientificname = "Mola mola", 
                               approach = "greedy", 
                               mon = "Aug")
cfg = read_configuration(version = "g_Aug")
db = brickman_database()
depth = read_brickman(db |> filter(scenario == "STATIC", var == "depth"))
covars = read_brickman(db |> filter(scenario == "PRESENT", interval == "mon"))
```

Of course we need covariates for August only, for this we can use a function we prepared earlier, `prep_model_data()`.  Note the we specifically ask for a plain table which means we are dropping the spatial information for now.

```{r august_data}
all_data = prep_model_data(model_input, 
                           month = "Aug",
                           covars = covars, 
                           depth = depth,
                           form = "table") 
all_data
```

# Split the data set into testing and training data sets

We will split out a random sample of our dataset to a larger set used for training the model, and a smaller set we withhold to use for later testing of the model.  Since we have labeled data ("presence" and "background") we want to be sure we sample these in proportion, for that we'll indicate that the data are stratified (into just two groups). Let's first determine what the proportion is before splitting.

```{r prop_variables}
# A little function to compute the ratio of presences to background
# @param x table with a "class" column
# @return numeric ratio presences/background
get_ratio = function(x){
  counts = count(x, class)
  np = filter(counts, class == "presence") |> pull(n)
  nb = filter(counts, class == "background") |> pull(n)
  return(np/nb)
}

cat("ratio of presence/background in full data set:", get_ratio(all_data), "\n")
```

Now let's make the split with the training set comprising 75% of `all_data`. Note that we specifically identify `class` as the `strata` (or grouping) variable.

```{r split}
split_data = initial_split(all_data, 
                           prop = 3/4,
                           strata = class)
split_data
```
It prints the counts of the training data, the testing data and the entire data set. We can extract the training data and testing data using the `training()` and `testing()` functions.  Let's check the ratios for those..

```{r check_strata}
cat("ratio of presence/background in training data:", 
    training(split_data) |> get_ratio(), "\n")

cat("ratio of presence/background in testing data:", 
    testing(split_data) |> get_ratio(), "\n")
```

OK! The samples observed the original proportion of presence/background.

> Note!  Did you notice that the function is called `initial_split()`, which implies a subsequent split - what do you suppose that is about? 

# Build a recipe

A recipe is a blueprint that guides the modeling process.

A recipe at a bare minimum needs to know two things: what is that data it has to work with and what is the relationship among the variables.  The latter is expressed as a formula, very similar to how we specify the formula of a line with `y = mx + b` or a parabola `y = ax^2 + bx + c`.

:::{.callout-note}

We often think of formulas as left-hand side (LHS) and right-hand side (RHS) equalities.  And usually, the LHS is the outcome while the RHS is about the inputs. For our modeling, the outcome is to predict the across the entire domain.  We can generalize the idea with the "is a function of" operator `~` (the tilde). For the classic formula for a line it like this... `y ~ x` and a parabola might be `y ~ x + x^2`

Consider a situation where we have reduced all of the suitabke variables to `Sbtm`, `Tbtm`, `MLD` and`Xbtm`, which we have in a table along with a `class` variable.
In our case we have the outcome is an prediction of `class` it is a function of variables like `Sbtm`, `Tbtm`, `MLD`, `Xbtm`, *etc.*  This formula would look like `y ~ Sbtm + Tbtm + MLD + Xbtm`.  Unlike the specific equation for a line or parabola, we don't pretend to know what coefficients, powers and that sort of stuff looks like.  We are just saying that `class` is a function of all of those variables (somehow).  

In the case here where the outcome (`class`) is a function of all other variables in the table, we have a nice short hand.  `class ~ .` where the dot means "every other variable".
:::

First we fish out of our split data the training data, and then drop the spatial information.

```{r tr_data}
tr_data = training(split_data)
tr_data
```

Now we make the recipe. Note that no computation takes place until we later `prep()`.
```{r recipe}
rec = recipe(class ~ ., data = tr_data)
rec
```
This print out provides a very high level summary - all of the details are glossed over.  To get a more detailed summary use the `summary()` function.

```{r recipe_summary}
summary(rec)
```

Each variable in the input is assigned a role: "outcome" or "predictor".  The latter are the variables used in the creation of the model. There are other types of roles, (see `?recipe`) including "case_weight" and "ID", and others can be defined as needed. Some are used in building the model, others are simply ride along and don't change the model outcome.

## Modifying the recipe with steps

Sometimes the recipes requires subsequent steps *before* the modeling begins in earnest.  Steps are cumulative, and that means the order in which tye are added matters. For example we know from experience that it is often useful to log scale (base 10) depth when working with biological models. Currently, `depth` ranges from `{r} sprintf("%0.1fm", min(all_data$depth))` to `{r} sprintf("%0.1fm",max(all_data$depth))`.  So, we'll add a step for that.

```{r step_log}
rec = rec |>
  step_log(depth, Xbtm,  base = 10)
rec
```

Next we state that we want to **remove** variables that might be haighly correlated with other variables. 

```{r}
rec = rec |> 
  step_corr(all_numeric_predictors())
rec
```
# Preparation for baking

Here we `prep()` the recipe - think of it as checking that you have all of the ingredients you need. It is here that the step operations are semi-implemented - a check is made on the `depth` variable (to make sure that log-scaling is possible), the means and standard deviations of variables are computed (if needed), checks for missing data, that kind of thing. If you added no preprocessing steps there is not much for the function to do, but it must be run.  

```{r prep}
rec = prep(rec)
rec
```

Now the steps have been semi-implemented, and the recipe is marked as trained.  It is possible to continue adding steps and run `prep()` again using the `refresh = TRUE` agument. But we are going onward to "bake" which actually applies the computations.  

# Bake the data

Here we use the information in the recipe and the original training data to apply any data transformations.

```{r bake}
baked_data = bake(rec, new_data = NULL)
baked_data
```
This is the data prepared with our specified transformations.  As luck has it, none of the variables were highly correlated so none were removed.  And `depth` is now log scaled. 

# Build a model

We are going to build a [random forest](https://www.geeksforgeeks.org/random-forest-algorithm-in-machine-learning/) model in classification mode which means for us that we have predictions of "presence" or "background". That's just two classes, random forests can predict multiple classes, too. Also, random forests can make regression models which are used for continuous data.  Below we start the model, declare its mode and assign an engine (the package we prefer to use.)  We'll be using the [ranger R pakage](http://imbs-hl.github.io/ranger/).

## Create the model
```{r start_rf}
model = rand_forest() |>
  set_mode("classification") |>
  set_engine("ranger", probability = TRUE, importance = "permutation") 
model
```
Well, that feels underwhelming. We can pass arguments unique to the engine using the `set_args()` function, but, for now we'll accept the defaults.


## Fit our specific model

```{r fit_rf}
model = model |>
  fit(formula(rec), data = tr_data)
model
```
# Making predictions with the model

Predicting is easy with this pattern: `predictions = predict(model, newdata, ...)`  We want to specify that we want probabilites of a particular class being predicted.  In each case we bind to the prediction our original classification, `class`.

## Predict with the training data

First we shall predict with the same data we trained with. The results of this will not really tell us much about our model as it is very circular to predict using the very data used to build the model.  So this next section is more about a first pass at using the tools at your disposal.

```{r predict_train}
train_pred = predict_model(model, tr_data, type = "prob")
train_pred
```

Here the variables prepended with a dot `.` are computed, while the `class` variable is our original. There are many metrics we can use to determine how well this model predicts.  Let's start with the simplest thing... we can make a simply tally of `.pred` and `class`.

```{r count_outcomes}
count(train_pred, .pred, class)
```
There false positives and false negatives, but many are correct. Of course, this is predicting with the very data we used to train the model; knowing that this is predicicting on training data with some many misses might not inspire confidence.  But let's explore more.

## Model metrics

### Confusion matrix

The confusion matrix is the next step beyond a simple tally that we made above.

```{r conf_mat_train}
train_confmat = conf_mat(train_pred, class, .pred)
train_confmat
```
It comes with handy plotting functionality.

```{r plot_confmat_train}
autoplot(train_confmat, type = "heatmap")
```

### ROC and AUC
The area under the curve ([AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve)) of the receiver-operator curve ([ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)) is a common metric.  AUC values range form 0-1 with 1 reflecting a model that faithfully predicts correctly. Technically, and AUC value of 0.5 represents a random model (yup, the result of a coin flip!). 

First we can plot the ROC.

```{r plot_roc}
plot_roc(train_pred, class, .pred_presence)
```

If you really only need the AUC, you can use the `roc_auc()` function directly.

```{r roc_auc}
roc_auc(train_pred, class,  .pred_presence)
```
### Accuracy

Accuracy, much like our simple tally above, tells us what fraction of the predictions are correct.  Not that here we explicitly provide the predicted class label (not the probability.)

```{r accuracy}
accuracy(train_pred, class, .pred)
```

### Partial dependence plot

Partial dependence reflects the relative contrubution of each variable influence over it's full range of values.  The output is a grid grid of plots showing the relative distribution of the variable (bars) as well as the relative inlfluence (line). 
```{r pd_plot}
partial_dependence_plot(model, data = tr_data)
```


## Predict with the testing data

Finally, we can repeat these steps with the testing data.  This should give use better information than using the training data.

### Predict

```{r predict_test}
test_data = testing(split_data)
test_pred = predict_model(model, test_data, type = "prob")
test_pred
```

### Confusion matrix

```{r conf_mat_test}
test_confmat = conf_mat(test_pred, class, .pred)
autoplot(test_confmat, type = "heatmap")
```

### ROC/AUC
```{r plot_roc_test}
plot_roc(test_pred, class, .pred_presence)
```
### Accuracy

```{r accuracy_test}
accuracy(test_pred, class, .pred)
```


### Partial Dependence
```{r pd_plot_test}
partial_dependence_plot(model, data = test_data)
```

# Saving a model to a file

We can (and possibly should!) save models to disk for later recall.  

```{r save_model}
write_model(model, version = cfg$version)
```

You can read it back later with `read_model()`.

# Recap

We have built a random forest model using some tools from the [tidymodels universe](https://www.tidymodels.org/).  After reading in a suite of data, we split our data into training and testing sets, witholding the testing set until the very end.  We looked a variety of metrics including a simple tally, a confusion matrix, ROC and AUC, accuracy and partial dependencies.  We saved the model to a file.

# Coding Assignment
::: {.callout-note appearance="simple"}


:::

