[
  {
    "objectID": "C04_models.html",
    "href": "C04_models.html",
    "title": "Ensembles",
    "section": "",
    "text": "All models are wrong, but some are useful.\n\nGeorge Box\nModeling starts with a collection of observations (presence and background for us!) and ends up with a collection of coeefficients that can be used with one or more formulas to make a predicition for the past, the present or the future. We are using modeling specifically to make habitat suitability maps for select species under two climate scenarios (RCP45 and RCP85) at two different times (2055 and 2075) in the future.\nWe can choose from a number of different models: random forest (rf), maximum entropy (maxent or maxnet), boosted regression trees (brt), general linear models (glm), etc. The point of each is to make a mathematical representation of natural occurrences. It is important to consider what those occurences might be - categorical like labels? likelihoods like probabilities? continuous like measurements? Here are examples of each…\nWe are modeling with known observations (presences) and a sampling of the background, so we are trying to model a likelihood that a species will be encountered (and reported) relative to the environmental conditions. So we are looking for a model that can produce relative likelihood of an encounter that results in a report.\nWe’ll be using a random forest model (rf), and we’ll be following this tidy models tutorial prepared by our colleague Omi Johnson.",
    "crumbs": [
      "Models"
    ]
  },
  {
    "objectID": "C04_models.html#modifying-the-recipe-with-steps",
    "href": "C04_models.html#modifying-the-recipe-with-steps",
    "title": "Ensembles",
    "section": "4.1 Modifying the recipe with steps",
    "text": "4.1 Modifying the recipe with steps\nSometimes the recipes requires subsequent steps before the modeling begins in earnest. Steps are cumulative, and that means the order in which tye are added matters. For example we know from experience that it is often useful to log scale (base 10) depth when working with biological models. Currently, depth ranges from 5.0m to 4081.5m. So, we’ll add a step for that.\n\nrec = rec |&gt;\n  step_log(depth, Xbtm,  base = 10)\nrec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 9\n\n\n\n\n\n── Operations \n\n\n• Log transformation on: depth and Xbtm\n\n\nNext we state that we want to remove variables that might be haighly correlated with other variables.\n\nrec = rec |&gt; \n  step_corr(all_numeric_predictors())\nrec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 9\n\n\n\n\n\n── Operations \n\n\n• Log transformation on: depth and Xbtm\n\n\n• Correlation filter on: all_numeric_predictors()",
    "crumbs": [
      "Models"
    ]
  },
  {
    "objectID": "C04_models.html#create-the-model",
    "href": "C04_models.html#create-the-model",
    "title": "Ensembles",
    "section": "7.1 Create the model",
    "text": "7.1 Create the model\n\nmodel = rand_forest() |&gt;\n  set_mode(\"classification\") |&gt;\n  set_engine(\"ranger\", probability = TRUE, importance = \"permutation\") \nmodel\n\nRandom Forest Model Specification (classification)\n\nEngine-Specific Arguments:\n  probability = TRUE\n  importance = permutation\n\nComputational engine: ranger \n\n\nWell, that feels underwhelming. We can pass arguments unique to the engine using the set_args() function, but, for now we’ll accept the defaults.",
    "crumbs": [
      "Models"
    ]
  },
  {
    "objectID": "C04_models.html#fit-our-specific-model",
    "href": "C04_models.html#fit-our-specific-model",
    "title": "Ensembles",
    "section": "7.2 Fit our specific model",
    "text": "7.2 Fit our specific model\n\nmodel = model |&gt;\n  fit(formula(rec), data = tr_data)\nmodel\n\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, probability = ~TRUE,      importance = ~\"permutation\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1)) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      5453 \nNumber of independent variables:  9 \nMtry:                             3 \nTarget node size:                 10 \nVariable importance mode:         permutation \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.2124971",
    "crumbs": [
      "Models"
    ]
  },
  {
    "objectID": "C04_models.html#predict-with-the-training-data",
    "href": "C04_models.html#predict-with-the-training-data",
    "title": "Ensembles",
    "section": "8.1 Predict with the training data",
    "text": "8.1 Predict with the training data\nFirst we shall predict with the same data we trained with. The results of this will not really tell us much about our model as it is very circular to predict using the very data used to build the model. So this next section is more about a first pass at using the tools at your disposal.\n\ntrain_pred = predict_model(model, tr_data, type = \"prob\")\ntrain_pred\n\n# A tibble: 5,453 × 4\n   .pred_presence .pred_background .pred      class     \n            &lt;dbl&gt;            &lt;dbl&gt; &lt;fct&gt;      &lt;fct&gt;     \n 1        0.0686             0.931 background background\n 2        0.0450             0.955 background background\n 3        0.107              0.893 background background\n 4        0.00585            0.994 background background\n 5        0.389              0.611 background background\n 6        0.0420             0.958 background background\n 7        0.0171             0.983 background background\n 8        0.0178             0.982 background background\n 9        0.0214             0.979 background background\n10        0.0530             0.947 background background\n# ℹ 5,443 more rows\n\n\nHere the variables prepended with a dot . are computed, while the class variable is our original. There are many metrics we can use to determine how well this model predicts. Let’s start with the simplest thing… we can make a simply tally of .pred and class.\n\ncount(train_pred, .pred, class)\n\n# A tibble: 4 × 3\n  .pred      class          n\n  &lt;fct&gt;      &lt;fct&gt;      &lt;int&gt;\n1 presence   presence    1393\n2 presence   background   345\n3 background presence     447\n4 background background  3268\n\n\nThere false positives and false negatives, but many are correct. Of course, this is predicting with the very data we used to train the model; knowing that this is predicicting on training data with some many misses might not inspire confidence. But let’s explore more.",
    "crumbs": [
      "Models"
    ]
  },
  {
    "objectID": "C04_models.html#model-metrics",
    "href": "C04_models.html#model-metrics",
    "title": "Ensembles",
    "section": "8.2 Model metrics",
    "text": "8.2 Model metrics\n\n8.2.1 Confusion matrix\nThe confusion matrix is the next step beyond a simple tally that we made above.\n\ntrain_confmat = conf_mat(train_pred, class, .pred)\ntrain_confmat\n\n            Truth\nPrediction   presence background\n  presence       1393        345\n  background      447       3268\n\n\nIt comes with handy plotting functionality.\n\nautoplot(train_confmat, type = \"heatmap\")\n\n\n\n\n\n\n\n\n\n\n8.2.2 ROC and AUC\nThe area under the curve (AUC) of the receiver-operator curve (ROC) is a common metric. AUC values range form 0-1 with 1 reflecting a model that faithfully predicts correctly. Technically, and AUC value of 0.5 represents a random model (yup, the result of a coin flip!).\nFirst we can plot the ROC.\n\nplot_roc(train_pred, class, .pred_presence)\n\n\n\n\n\n\n\n\nIf you really only need the AUC, you can use the roc_auc() function directly.\n\nroc_auc(train_pred, class,  .pred_presence)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.942\n\n\n\n\n8.2.3 Accuracy\nAccuracy, much like our simple tally above, tells us what fraction of the predictions are correct. Not that here we explicitly provide the predicted class label (not the probability.)\n\naccuracy(train_pred, class, .pred)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.855\n\n\n\n\n8.2.4 Partial dependence plot\nPartial dependence reflects the relative contrubution of each variable influence over it’s full range of values. The output is a grid grid of plots showing the relative distribution of the variable (bars) as well as the relative inlfluence (line).\n\npartial_dependence_plot(model, data = tr_data)",
    "crumbs": [
      "Models"
    ]
  },
  {
    "objectID": "C04_models.html#predict-with-the-testing-data",
    "href": "C04_models.html#predict-with-the-testing-data",
    "title": "Ensembles",
    "section": "8.3 Predict with the testing data",
    "text": "8.3 Predict with the testing data\nFinally, we can repeat these steps with the testing data. This should give use better information than using the training data.\n\n8.3.1 Predict\n\ntest_data = testing(split_data)\ntest_pred = predict_model(model, test_data, type = \"prob\")\ntest_pred\n\n# A tibble: 1,819 × 4\n   .pred_presence .pred_background .pred      class   \n            &lt;dbl&gt;            &lt;dbl&gt; &lt;fct&gt;      &lt;fct&gt;   \n 1          0.459           0.541  background presence\n 2          0.680           0.320  presence   presence\n 3          0.397           0.603  background presence\n 4          0.627           0.373  presence   presence\n 5          0.168           0.832  background presence\n 6          1               0      presence   presence\n 7          0.658           0.342  presence   presence\n 8          0.976           0.0244 presence   presence\n 9          1               0      presence   presence\n10          0.461           0.539  background presence\n# ℹ 1,809 more rows\n\n\n\n\n8.3.2 Confusion matrix\n\ntest_confmat = conf_mat(test_pred, class, .pred)\nautoplot(test_confmat, type = \"heatmap\")\n\n\n\n\n\n\n\n\n\n\n8.3.3 ROC/AUC\n\nplot_roc(test_pred, class, .pred_presence)\n\n\n\n\n\n\n\n\n\n\n8.3.4 Accuracy\n\naccuracy(test_pred, class, .pred)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.697\n\n\n\n\n8.3.5 Partial Dependence\n\npartial_dependence_plot(model, data = test_data)",
    "crumbs": [
      "Models"
    ]
  }
]