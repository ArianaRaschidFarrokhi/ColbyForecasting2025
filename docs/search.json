[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Brought to you by the Tandy Center for Ocean Forecasting at Bigelow Laboratory for Ocean Science and Colby College.\nContacts:\nDr. Nick Record Ben Tupper\nRaising questions or issues: If you have a question, start a new “issue” on the github issues tab. If a question has been posed by another, and you think you can help with the answer then please feel free to respond.\n\n\n\n Back to top",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "C03_covariates.html",
    "href": "C03_covariates.html",
    "title": "Covariates",
    "section": "",
    "text": "“In the end that was the choice you made, and it doesn’t matter how hard it was to make it. It matters that you did.”\n\nCassandra Clare\nNow we turn our attention to what we know and guess about the environments. We are using the Brickman data to make habitat suitability maps for select species under two climate scenarios (RCP45 and RCP85) at two different times (2055 and 2075) in the future. Each variable we might use is called covariate or predictor. Our covariates are nicely packaged up and tidy, but the reality is that it often requires a good deal of data wrangling if the data are messy.\nOur step here is to make sure that two or more covariates are not highly correlated if they are, then we would likely want to drop all but one.",
    "crumbs": [
      "Covariates"
    ]
  },
  {
    "objectID": "C03_covariates.html#reading-in-the-covariates",
    "href": "C03_covariates.html#reading-in-the-covariates",
    "title": "Covariates",
    "section": "2.1 Reading in the covariates",
    "text": "2.1 Reading in the covariates\nWe’ll read in the Brickman database, then filter two different subsets to read: “STATIC” covariate bathymetry that apply across all scenarios and times and monthly covariates for the “PRESENT” period.\n\ndb = brickman_database()\ndepth = read_brickman(filter(db, var == 'depth'))\npresent = read_brickman(filter(db, scenario == \"PRESENT\", interval == \"mon\"))\n\nWe should pick one month, and join that month’s model data with depth. We have used August before as our example, let’s continue with August.\n\naug = present |&gt;\n  dplyr::slice(\"month\", \"Aug\")\naug = bind_attrs(aug, depth)",
    "crumbs": [
      "Covariates"
    ]
  },
  {
    "objectID": "C03_covariates.html#make-a-pairs-plot",
    "href": "C03_covariates.html#make-a-pairs-plot",
    "title": "Covariates",
    "section": "2.2 Make a pairs plot",
    "text": "2.2 Make a pairs plot\nA pairs plot is a plot often used in exploratory data analysis. It makes a grid of mini-plots of a set of variables, and reveals the relationships among the variables pair-by-pair. It’s easy to make.\n\npairs(aug)\n\n\n\n\n\n\n\n\nIn the lower left portion of the plot we see paired scatter plots, at upper right we see the correlation values of the pairs, and long the diagonal we see a histogram of each variable. Some pairs are highly correlated, say over 0.7, and to include both in the modeling might not provide us with greater predictive power. It may feel counterintuitive to remove any variables - more data means more information, right? And more information means more informed models. Consider two measurements, human height and inseam. We might use these to predict if a person can dunk a basketball, but since they are probably strongly correlated do you really need both?",
    "crumbs": [
      "Covariates"
    ]
  },
  {
    "objectID": "C03_covariates.html#identify-the-most-independent-variables-and-the-most-collinear",
    "href": "C03_covariates.html#identify-the-most-independent-variables-and-the-most-collinear",
    "title": "Covariates",
    "section": "2.3 Identify the most independent variables (and the most collinear)",
    "text": "2.3 Identify the most independent variables (and the most collinear)\nWe have a function that can help use select which variables to remove. filter_collinear() returns a listing of variables it suggests we keep. It attaches to the return value an attribute (like a post-it note stuck on a box) that lists the complementary variables that it suggests we drop. We are choosing a particular method you can learn more about using R’s help ?filter_collinear.\n\nkeep = filter_collinear(aug, method = \"vif_step\")\nkeep\n\n[1] \"MLD\"  \"Sbtm\" \"SSS\"  \"SST\"  \"Tbtm\" \"U\"    \"V\"   \nattr(,\"to_remove\")\n[1] \"Xbtm\"  \"depth\"\n\n\nOf course, we can decide to ignore this advice, and pick which ever ones we want including keeping them all.\nWhatever selection of variable we decide to model with, we will save this listing to a file. That way we can refer to it progammatically. But that comes later.",
    "crumbs": [
      "Covariates"
    ]
  },
  {
    "objectID": "C03_covariates.html#a-closer-look-at-the-model-input-data",
    "href": "C03_covariates.html#a-closer-look-at-the-model-input-data",
    "title": "Covariates",
    "section": "2.4 A closer look at the model input data",
    "text": "2.4 A closer look at the model input data\nBefore we do commit to a selection of variables, let’s turn our attention back to our presence-background points, and look at just those chosen values rather than at values drawn form across the entire domain. Let’s open the file that contains the “greedy” model input for August during the PRESENT climate scenario.\n\nmodel_input = read_model_input(scientificname = \"Mola mola\", \n                               approach = \"greedy\", \n                               mon = \"Aug\")\nmodel_input\n\nSimple feature collection with 7277 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -74.89169 ymin: 38.805 xmax: -65.02004 ymax: 45.21401\nGeodetic CRS:  WGS 84\n# A tibble: 7,277 × 2\n   class                    geom\n   &lt;chr&gt;             &lt;POINT [°]&gt;\n 1 presence    (-72.8074 39.056)\n 2 presence      (-71.343 40.52)\n 3 presence  (-68.7691 41.52448)\n 4 presence       (-67.79 43.32)\n 5 presence (-68.44324 42.61177)\n 6 presence    (-72.4328 40.213)\n 7 presence   (-71.8784 40.3569)\n 8 presence      (-65.78 43.195)\n 9 presence       (-70.5 42.767)\n10 presence   (-72.3024 40.1862)\n# ℹ 7,267 more rows\n\n\nNext we’ll extract data values from our August covariates.\n\nvariables = extract_brickman(aug, model_input, form = \"wide\")\nvariables\n\n# A tibble: 7,277 × 10\n   point   MLD  Sbtm   SSS   SST  Tbtm         U         V     Xbtm depth\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 p0001  5.17  35.0  31.6  23.3  7.50 -0.00161  -0.00340  0.00133  304. \n 2 p0002  4.25  32.8  30.6  21.6  8.15 -0.00420  -0.00206  0.00166   71.6\n 3 p0003  4.64  34.0  30.7  20.2  7.05  0.00168   0.00148  0.000793 138. \n 4 p0004  5.58  34.6  30.7  18.8  7.55  0.00267  -0.000410 0.000957 234. \n 5 p0005  5.04  34.7  30.7  19.0  7.43 -0.00619  -0.00121  0.00224  205. \n 6 p0006  4.01  32.4  30.6  22.0  8.22 -0.00344  -0.000859 0.00126   62.6\n 7 p0007  4.10  32.9  30.5  21.8  8.34 -0.00565  -0.00226  0.00216   71.3\n 8 p0008  3.82  32.4  30.3  18.2  3.56 -0.00702  -0.00431  0.00293   81.6\n 9 p0009  3.20  32.4  30.6  17.9  5.73  0.000275 -0.00101  0.000372  70.6\n10 p0010  4.02  32.9  30.6  22.0  8.62 -0.000900 -0.00148  0.000614  64.9\n# ℹ 7,267 more rows\n\n\nWe are going to call a plotting function, plot_pres_vs_bg(), that wants some of the data from model_input and some of the data in variables. So, we have to do some data wrangling to combine those; we’ll add class to variables and then drop the point column.\n\nvariables = variables |&gt;\n  mutate(class = model_input$class) |&gt;    # the $ extracts a column \n  select(-point)                          # the - means \"deselect\" or \"drop\"\nvariables\n\n# A tibble: 7,277 × 10\n     MLD  Sbtm   SSS   SST  Tbtm         U         V     Xbtm depth class   \n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   \n 1  5.17  35.0  31.6  23.3  7.50 -0.00161  -0.00340  0.00133  304.  presence\n 2  4.25  32.8  30.6  21.6  8.15 -0.00420  -0.00206  0.00166   71.6 presence\n 3  4.64  34.0  30.7  20.2  7.05  0.00168   0.00148  0.000793 138.  presence\n 4  5.58  34.6  30.7  18.8  7.55  0.00267  -0.000410 0.000957 234.  presence\n 5  5.04  34.7  30.7  19.0  7.43 -0.00619  -0.00121  0.00224  205.  presence\n 6  4.01  32.4  30.6  22.0  8.22 -0.00344  -0.000859 0.00126   62.6 presence\n 7  4.10  32.9  30.5  21.8  8.34 -0.00565  -0.00226  0.00216   71.3 presence\n 8  3.82  32.4  30.3  18.2  3.56 -0.00702  -0.00431  0.00293   81.6 presence\n 9  3.20  32.4  30.6  17.9  5.73  0.000275 -0.00101  0.000372  70.6 presence\n10  4.02  32.9  30.6  22.0  8.62 -0.000900 -0.00148  0.000614  64.9 presence\n# ℹ 7,267 more rows\n\n\nFinally, can make a specialized plot comparing our variables for each class: presence and background.\n\nplot_pres_vs_bg(variables, \"class\")\n\n\n\n\n\n\n\n\nHow does this inform our thinking about reducing the number of variables? For which variables do presence and background values mirror each other? Which have the least overlap? We know that the model works by finding optimal combinations of covariates for the species. If there is never a difference between the conditions for presences and background then how will it find the optimal niche conditions?",
    "crumbs": [
      "Covariates"
    ]
  },
  {
    "objectID": "C03_covariates.html#saving-a-file-to-keep-track-of-modeling-choices",
    "href": "C03_covariates.html#saving-a-file-to-keep-track-of-modeling-choices",
    "title": "Covariates",
    "section": "2.5 Saving a file to keep track of modeling choices",
    "text": "2.5 Saving a file to keep track of modeling choices\nYou may have noticed that we write a lot of things to files (aka, “writing to disk”). It’s a useful practice especially when working with a multi-step process. One particular file, a configuration file, is used frequently in data science to store information about the choices we make as we work through our project. Configuration files generally are simple text files that we can easily get the computer to read and write.\nIn R, a confguration is treated as a named list. Each element of a list is named, but beyond that there aren’t any particular rules about confugurations. You can learn more about configurations in this tutorial.\nLet’s make a confuguration list that holds 4 items: version identifier, species name, sampling approach and the names of the variables to model with.\n\ncfg = list(\n  version = \"g_Aug\",               # g for greedy!\n  scientificname = \"Mola mola\",\n  approach = \"greedy\",\n  mon = \"Aug\",\n  keep_vars =  keep)\n\nWe can access by name three ways using what is called “indexing” : using the [[ indexing brackets, using the $ indexing operator or using the getElement() function.\n\ncfg[['scientificname']]\n\n[1] \"Mola mola\"\n\ncfg[[2]]\n\n[1] \"Mola mola\"\n\ncfg$scientificname\n\n[1] \"Mola mola\"\n\ngetElement(cfg, \"scientificname\")\n\n[1] \"Mola mola\"\n\ngetElement(cfg, 2)\n\n[1] \"Mola mola\"\n\n\nNow we’ll write this list to a file. First let’s set up a pathwy where we might store these configurations, and for that matter, to store our modeling files. We’ll make a new directory, models/g008 and write the configuration there. We’ll use the famous “YAML” format to store the file. See the file functions/configuration.R for documentation on reading and writing.\n\nok = make_path(data_path(\"models\")) # make a directory for models\nwrite_configuration(cfg)            \n\nUse the Files pane to navigate to your personal data directory. Open the g_Aug.yaml file - this is what you configuration looks like in YAML. Fortunately we don’t mess manually with these much.",
    "crumbs": [
      "Covariates"
    ]
  },
  {
    "objectID": "C01_observations.html",
    "href": "C01_observations.html",
    "title": "Observations",
    "section": "",
    "text": "Follow this wiki page on obtaining data from OBIS. Keep in mind that you will probably want a species with sufficient number of records in the northwest Atlantic. Just what constitutes “sufficient” is probably subject to some debate, but a couple of hundred as a minumum will be helpful for learning. One thing that might help is to be on alert species that are only congregate in one area such as right along the shoreline or only appear in a few months of the year. It isn’t that those species are not worthy of study, but they may make the learning process harder.\nYou should feel free to get the data for a couple of different species, if one becomes a headache with our given resources, then you can switch easily to another.",
    "crumbs": [
      "Observations"
    ]
  },
  {
    "objectID": "C01_observations.html#basisofrecord",
    "href": "C01_observations.html#basisofrecord",
    "title": "Observations",
    "section": "5.1 basisOfRecord",
    "text": "5.1 basisOfRecord\nNext we should examine the basisOfRecord variable to get an understanding of how these observations were made.\n\nobs |&gt; count(basisOfRecord)\n\nSimple feature collection with 4 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: -74.65 ymin: 38.8 xmax: -65.00391 ymax: 45.1333\nGeodetic CRS:  WGS 84\n# A tibble: 4 × 3\n  basisOfRecord              n                                              geom\n* &lt;chr&gt;                  &lt;int&gt;                                    &lt;GEOMETRY [°]&gt;\n1 HumanObservation        9354 MULTIPOINT ((-65.07 42.68), (-65.067 42.65), (-6…\n2 NomenclaturalChecklist     1                        POINT (-65.80602 44.97985)\n3 Occurrence                 1                          POINT (-65.2852 42.6243)\n4 PreservedSpecimen        170 MULTIPOINT ((-67.05534 45.09908), (-66.35 45.133…\n\n\nIf you are using a different species you may have different values for basisOfRecord. Let’s take a closer look at the complete records for one from each group.\n\nhuman = obs |&gt;\n  filter(basisOfRecord == \"HumanObservation\") |&gt;\n  slice(1) |&gt;\n  browse_obis()\n\nPlease point your browser to the following url: \n\n\nhttps://api.obis.org/v3/occurrence/00040fa1-7acd-4731-bf1e-6dc16e30c7d4\n\npreserved = obs |&gt;\n  filter(basisOfRecord == \"PreservedSpecimen\") |&gt;\n  slice(1) |&gt;\n  browse_obis()\n\nPlease point your browser to the following url: \n\n\nhttps://api.obis.org/v3/occurrence/003abd48-a98a-4c2f-adc2-8f1d6f71dfa1\n\nchecklist = obs |&gt;\n  filter(basisOfRecord == \"NomenclaturalChecklist\") |&gt;\n  slice(1) |&gt;\n  browse_obis()\n\nPlease point your browser to the following url: \n\n\nhttps://api.obis.org/v3/occurrence/1b967631-4d90-44d0-b57e-cf71c554ee5c\n\noccurrence = obs |&gt;\n  filter(basisOfRecord == \"Occurrence\") |&gt;\n  slice(1) |&gt;\n  browse_obis()\n\nPlease point your browser to the following url: \n\n\nhttps://api.obis.org/v3/occurrence/d6e7882e-a850-435d-a546-73adaf625031\n\n\nNext let’s think about what our minimum requirements might be in oirder to build a model. To answer that we need to think about our environmental covariates in the Brickman data](https://github.com/BigelowLab/ColbyForecasting2025/wiki/Brickman). That data has dimensions of x (longitude), y (latitude) and month. In order to match obseravtions with that data, our observations must be complete in those three variables. Let’s take a look at a summary of the observations which will indicate the number of elements missing in each variable.\n\nsummary(obs)\n\n      id            basisOfRecord        eventDate               year     \n Length:9526        Length:9526        Min.   :1932-09-15   Min.   :1932  \n Class :character   Class :character   1st Qu.:2003-10-02   1st Qu.:2003  \n Mode  :character   Mode  :character   Median :2009-07-11   Median :2009  \n                                       Mean   :2006-10-02   Mean   :2006  \n                                       3rd Qu.:2016-11-05   3rd Qu.:2016  \n                                       Max.   :2021-10-14   Max.   :2021  \n                                       NA's   :7            NA's   :7     \n    month            eventTime         individualCount             geom     \n Length:9526        Length:9526        Min.   : 1.000   POINT        :9526  \n Class :character   Class :character   1st Qu.: 1.000   epsg:4326    :   0  \n Mode  :character   Mode  :character   Median : 1.000   +proj=long...:   0  \n                                       Mean   : 1.112                       \n                                       3rd Qu.: 1.000                       \n                                       Max.   :25.000                       \n                                       NA's   :318",
    "crumbs": [
      "Observations"
    ]
  },
  {
    "objectID": "C01_observations.html#eventdate",
    "href": "C01_observations.html#eventdate",
    "title": "Observations",
    "section": "5.2 eventDate",
    "text": "5.2 eventDate\nFor Mola mola there are some rows where eventDate is NA. We need to filter those. The filter function looks for a vector of TRUE/FALSE values - one for each row. In our case, we test the eventDate column to see if it is NA, but then we reverse the TRUE/FALSE logical with the preceding ! (pronounded “bang!”). This we retain only the rows where eventDate is notNA`, and then we print the summary again.\n\nobs = obs |&gt;\n  filter(!is.na(eventDate))\nsummary(obs)\n\n      id            basisOfRecord        eventDate               year     \n Length:9519        Length:9519        Min.   :1932-09-15   Min.   :1932  \n Class :character   Class :character   1st Qu.:2003-10-02   1st Qu.:2003  \n Mode  :character   Mode  :character   Median :2009-07-11   Median :2009  \n                                       Mean   :2006-10-02   Mean   :2006  \n                                       3rd Qu.:2016-11-05   3rd Qu.:2016  \n                                       Max.   :2021-10-14   Max.   :2021  \n                                                                          \n    month            eventTime         individualCount             geom     \n Length:9519        Length:9519        Min.   : 1.000   POINT        :9519  \n Class :character   Class :character   1st Qu.: 1.000   epsg:4326    :   0  \n Mode  :character   Mode  :character   Median : 1.000   +proj=long...:   0  \n                                       Mean   : 1.112                       \n                                       3rd Qu.: 1.000                       \n                                       Max.   :25.000                       \n                                       NA's   :315",
    "crumbs": [
      "Observations"
    ]
  },
  {
    "objectID": "C01_observations.html#individualcount",
    "href": "C01_observations.html#individualcount",
    "title": "Observations",
    "section": "5.3 individualCount",
    "text": "5.3 individualCount\nThat’s better, but we still have 315 NA values for individualCount. Let’s look at at least one record of those in detail; filter out one, and browse it.\n\nobs |&gt;\n  filter(is.na(individualCount)) |&gt;\n  slice(1) |&gt;\n  browse_obis()\n\nPlease point your browser to the following url: \n\n\nhttps://api.obis.org/v3/occurrence/003abd48-a98a-4c2f-adc2-8f1d6f71dfa1\n\n\nEeek! It’s a carcas that washed up on shore! We checked a number of others, and they are all carcases. Is that a presence? Is that what we model are modeling? If not then we should filer those out.\n\nobs = obs |&gt;\n  filter(!is.na(individualCount))\nsummary(obs)\n\n      id            basisOfRecord        eventDate               year     \n Length:9204        Length:9204        Min.   :1932-09-15   Min.   :1932  \n Class :character   Class :character   1st Qu.:2003-07-26   1st Qu.:2003  \n Mode  :character   Mode  :character   Median :2009-07-11   Median :2009  \n                                       Mean   :2006-08-17   Mean   :2006  \n                                       3rd Qu.:2016-11-05   3rd Qu.:2016  \n                                       Max.   :2021-10-14   Max.   :2021  \n    month            eventTime         individualCount             geom     \n Length:9204        Length:9204        Min.   : 1.000   POINT        :9204  \n Class :character   Class :character   1st Qu.: 1.000   epsg:4326    :   0  \n Mode  :character   Mode  :character   Median : 1.000   +proj=long...:   0  \n                                       Mean   : 1.112                       \n                                       3rd Qu.: 1.000                       \n                                       Max.   :25.000                       \n\n\nWell now one has to wonder about a single observation of 25 animals. Let’s check that out.\n\nobs |&gt;\n  filter(individualCount == 25) |&gt;\n  browse_obis()\n\nPlease point your browser to the following url: \n\n\nhttps://api.obis.org/v3/occurrence/c907349a-2c52-4a51-a69a-5a338c5d492a\n\n\nOK, that seems legitmate. And it is possible, Mola mola can congregate for feeding, mating and possibly for karaoke parties.",
    "crumbs": [
      "Observations"
    ]
  },
  {
    "objectID": "C01_observations.html#year",
    "href": "C01_observations.html#year",
    "title": "Observations",
    "section": "5.4 year",
    "text": "5.4 year\nWe know that the “current” climate scenario for the Brickman model data define “current” as the 1982-2013 window. It’s just an average, and if you have values from 1970 to the current year, you probably are safe in including them. But do your observations fall into those years? Let’s make a plot of the counts per year, with dashed lines shown the Brickman “current” cliamtology period.\n\nggplot(data = obs,\n       mapping = aes(x = year)) + \n  geom_bar() + \n  geom_vline(xintercept = c(1982, 2013), linetype = \"dashed\") + \n  labs(title = \"Counts per year\")\n\n\n\n\n\n\n\n\nFor this species, it seem like it is only the record from 1932 that might be a stretch, so let’s filter that out by rejecting records before 1970. This time, instead of asking for a sumamry, we’ll print the dimensions (rows, columns) of the table.\n\nobs = obs |&gt;\n  filter(year &gt;= 1970)\ndim(obs)\n\n[1] 9203    8\n\n\nThat’s still a lot of records. Now let’s check out the distribution across the months of the year.",
    "crumbs": [
      "Observations"
    ]
  },
  {
    "objectID": "C01_observations.html#month",
    "href": "C01_observations.html#month",
    "title": "Observations",
    "section": "5.5 month",
    "text": "5.5 month\nWe will be making models and predictions for each month of the for the 4 future projection climates. Species and observers do show some seasonality, but it that seasonality so extreme that it might be impossible to model some months because of sparse data? Let’s make a plot of the counts per month.\n\nggplot(data = obs,\n       mapping = aes(x = month)) + \n  geom_bar() + \n  labs(title = \"Counts per month\")\n\n\n\n\n\n\n\n\nOh, rats! By default ggplot plots in alpha-numeric order, which scrambles our month order. To fix that we have to convert the month in a factor type while specifying the order of the factors, and we’ll use the mutate() function to help us.\n\nobs = obs |&gt;\n  mutate(month = factor(month, levels = month.abb))\n\nggplot(data = obs,\n       mapping = aes(x = month)) + \n  geom_bar() + \n  labs(title = \"Counts per month\")\n\n\n\n\n\n\n\n\nThat’s better! So, it may be the for Mola mola we might not be able to successfully model in the cold winter months. That’s good to keep in mind.",
    "crumbs": [
      "Observations"
    ]
  },
  {
    "objectID": "C01_observations.html#geometry",
    "href": "C01_observations.html#geometry",
    "title": "Observations",
    "section": "5.6 geometry",
    "text": "5.6 geometry\nLast, but certainly not least, we should consider the possibility that some observations might be on shore. It happens! We already know that some records included fish that were washed up on shore. It’s possible someone mis-keyed the longitude or latitude when entering the vaklues into the database. It’s alos possible that some observations fall just outside the areas where the Brickman data has values. To look for these points, we’ll load the Brickman mask (defines land vs water. Well, really it defines data vs no-data), and use that for further filtering.\nWe need to load the Brickman database, and then filter it for the static variable called “mask”.\n\ndb = brickman_database() |&gt;\n  filter(scenario == \"STATIC\", var == \"mask\")\nmask = read_brickman(db)\nmask\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n      Min. 1st Qu. Median Mean 3rd Qu. Max. NA's\nmask     1       1      1    1       1    1 4983\ndimension(s):\n  from  to offset    delta refsys point x/y\nx    1 121 -74.93  0.08226 WGS 84 FALSE [x]\ny    1  89  46.08 -0.08226 WGS 84 FALSE [y]\n\n\nLet’s see what our mask looks like with the observations drizzled on top. Because the mask only has values of 1 (data) or NA (no-data). You’ll note that we only want to plot the locations of the observations, so we strip obs of everyhting except its geometery.\n\nplot(mask, breaks = \"equal\", axes = TRUE, reset = FALSE)\nplot(st_geometry(obs), pch = \".\", add = TRUE)\n\n\n\n\n\n\n\n\nMaybe with proper with squinting we can see some that faal into no-data areas. The sure-fire way to tell is to extract the mask values at the point locations.\n\nhitOrMiss = extract_brickman(mask, obs)\nhitOrMiss\n\n# A tibble: 9,203 × 3\n   point name  value\n   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n 1 p0001 mask      1\n 2 p0002 mask      1\n 3 p0003 mask      1\n 4 p0004 mask      1\n 5 p0005 mask      1\n 6 p0006 mask      1\n 7 p0007 mask      1\n 8 p0008 mask      1\n 9 p0009 mask      1\n10 p0010 mask      1\n# ℹ 9,193 more rows\n\n\nOK, let’s tally the “value” variable.\n\ncount(hitOrMiss, value)\n\n# A tibble: 2 × 2\n  value     n\n  &lt;dbl&gt; &lt;int&gt;\n1     1  9170\n2    NA    33\n\n\nOoooo, 33 records in obs don’t line up with values in the mask (or in any Brickman data). We should filter those out; we’ll do so with a filter(). Note that we a “reaching” into the hitOrMiss table to access the value column when we use this hitOrMiss$value. Let’s figure out how many records we have dropped with all of this filtering.\n\nobs = obs |&gt;\n  filter(!is.na(hitOrMiss$value))\ndim_end = dim(obs)\n\ndropped_records = dim_start[1] - dim_end[1]\ndropped_records\n\n[1] 356\n\n\nSo, we dropped 356 records which is about 3.7% of the raw OBIS data. Is it worth all that to drop just 4% of the data? Yes! Models are like all things computer… if you put garbage in you should expect to get garbage back out.",
    "crumbs": [
      "Observations"
    ]
  },
  {
    "objectID": "F00_forecasting.html",
    "href": "F00_forecasting.html",
    "title": "Forecasting",
    "section": "",
    "text": "Add pages names FXX_description.qmd to make your life easier. (Coding sections will be CXX_description.qmd).\nEdit _quarto.yaml in two sections: website &gt; sidebar and project &gt; render as you add pages.\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.",
    "crumbs": [
      "Forecasting"
    ]
  },
  {
    "objectID": "F00_forecasting.html#nick-stuff-goes-here.",
    "href": "F00_forecasting.html#nick-stuff-goes-here.",
    "title": "Forecasting",
    "section": "",
    "text": "Add pages names FXX_description.qmd to make your life easier. (Coding sections will be CXX_description.qmd).\nEdit _quarto.yaml in two sections: website &gt; sidebar and project &gt; render as you add pages.\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.",
    "crumbs": [
      "Forecasting"
    ]
  },
  {
    "objectID": "F00_forecasting.html#running-code",
    "href": "F00_forecasting.html#running-code",
    "title": "Forecasting",
    "section": "2 Running Code",
    "text": "2 Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed).",
    "crumbs": [
      "Forecasting"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Colby Forecasting",
    "section": "",
    "text": "Welcome to the Colby Forecasting 2025 workbook!\nThis document is comprised of sections: forecasting and coding with R programming language.\n\n1 Contacts:\nDr. Nick Record and Ben Tupper\n\n\n2 Questions and issues\nWe have a saying at Bigelow Lab, “there’s no such thing as a dumb question, but the quality of the answers you get may vary widely.” This is so true!\nIf you have a class, coding or forecasting question, start a new “issue” on the github issues tab. If a question has been posed by another, and you think you can help with the answer, then please feel free to respond. If you have a personal question or issue, then contact the instructors directly.\n\n\n3 The wiki\nSome ancillary content for the course has been placed in what is called a wiki. In theory anyone can contribute to a wiki, but in practice only a few do. We are open to suggestions for improvements and additions.\n\n\n\n\n Back to top",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "C00_coding.html",
    "href": "C00_coding.html",
    "title": "Coding",
    "section": "",
    "text": "Coding is the practice of writing instructions for computers to follow; computers aren’t clever by themselves - they need to be told what to do. Most coding is text-based; people writing coding instructions into simple text documents. But some coding is graphical or visual. We shall be using text-based coding. We are going to use a free and open source general programming language called R. R programming language has its roots in statistics and science, but it really can be used for anything.\nIn the early days, coding was pretty barebones - all one needed was a text editor and access to the programming language - no frills there, no pretty images, no buttons to push, just typing. As time passed, volunteers added niceties to the text editor, like visualizing plots of data, buttons to save files, colorized text for the typed code, and other bells and whistles. These editors became know as graphical user interfaces (GUI for short.) GUIs keep getting easier and easier for people to use. We will use the GUI known as RStudio. It’s best to think of GUIs as wrappers around the core programming language; they are really nice and pretty, but they can’t do math. The programming language itself (which does do math!), evolved only as it needed to to fix bugs and make general improvements.",
    "crumbs": [
      "Coding"
    ]
  },
  {
    "objectID": "C00_coding.html#loading-the-necessary-tools",
    "href": "C00_coding.html#loading-the-necessary-tools",
    "title": "Coding",
    "section": "4.1 Loading the necessary tools",
    "text": "4.1 Loading the necessary tools\nFor any coding project you will need to access a select number of tools, often stored on your computer in what is called a package library (it’s just a directory/folder really). When the package is loaded from the library, all of the functionality the author built in to that package is exposed for you to use in your project. We have created a single file that will both install (if needed) and load (if not already loaded) each of these packages. It’s easy to run.\nFirst, make sure that you have loaded the project (File &gt; Open Project) if you haven’t already. Then at the R console pane type the following…\n\nsource(\"setup.R\")\n\nAfter a few moments the command prompt will return to focus. Be sure to run that command at the beginning of every new R session or anytime you are adding new functionality.\nNow we are ready to load some data into your R session.",
    "crumbs": [
      "Coding"
    ]
  },
  {
    "objectID": "C00_coding.html#spatial-data",
    "href": "C00_coding.html#spatial-data",
    "title": "Coding",
    "section": "4.2 Spatial data",
    "text": "4.2 Spatial data\nSpatial data is any data that has been assigned to a location on a planet (or even between planets!); that means environmental data is mapped to locations on oblate spheroids (like Earth). The oblate spheroid shape presents interesting but challenging math to the data scientist. Modern spatial data is designed to make data science easier by handling all of the location information in a discrete and standardized manner. By discrete we mean that we don’t have to sweat the details.\n\n4.2.1 Point data\nMany spatial data sets come as point data - locations (longitude, latitude and maybe altitude/depth and/or time) with one or more measurements (temperature, cloudiness, probability of precipitation, abundance of fish, population density, etc) attached to that point. Here is an example of point data about long-term oceanographic monitoring buoys in the Gulf of Maine (“gom”). We’ll read the buoy data into a variable, buoy. Next we can print the result simply by typing the name (or you could type print(buoys) if you like all the extra typing.)\n\nbuoys = gom_buoys()\nbuoys\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -70.4277 ymin: 42.3233 xmax: -65.9267 ymax: 44.10163\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 4\n  name  longname            id                geometry\n* &lt;chr&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;POINT [°]&gt;\n1 wms   Western Maine Shelf B01    (-70.4277 43.18065)\n2 cms   Central Maine Shelf E01     (-69.3578 43.7148)\n3 pb    Penobscot Bay       F01   (-68.99689 44.05495)\n4 ems   Eastern Maine Shelf I01   (-68.11359 44.10163)\n5 jb    Jordan Basin        M01   (-67.88029 43.49041)\n6 nec   Northeast Channel   N01     (-65.9267 42.3233)\n\n\nSo there are 6 buoys, each with an attached attribute “name”, “longname” and “id”, as well as the spatial location datain the “geometry” column (just longitude and latitude in this case). We can easily plot these using the “name” column as a color key. For more on plotting spatial data, see this wiki page.\n\nplot(buoys['id'], axes = TRUE, pch = 16)\n\n\n\n\n\n\n\n\nWell, that’s pretty, but without a shoreline it lacks context.",
    "crumbs": [
      "Coding"
    ]
  },
  {
    "objectID": "C00_coding.html#linestrings-and-polygon-data",
    "href": "C00_coding.html#linestrings-and-polygon-data",
    "title": "Coding",
    "section": "4.3 Linestrings and polygon data",
    "text": "4.3 Linestrings and polygon data\nLinestrings (open shapes) and polygons (closed shape) are much like point data, except that each geometry is linestring or polygon. We have a set of polygons/linestring that represent the coastline.\n\ncoast = read_coastline()\ncoast\n\nSimple feature collection with 14 features and 0 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -74.9 ymin: 38.95218 xmax: -65 ymax: 46.06477\nGeodetic CRS:  WGS 84\n# A tibble: 14 × 1\n                                                                            geom\n                                                           &lt;MULTILINESTRING [°]&gt;\n 1 ((-72.1019 41.01504, -72.15127 41.05146, -72.18389 41.04678, -72.28745 41.02…\n 2 ((-73.68745 45.56143, -73.85293 45.51572, -73.96055 45.44141, -73.92021 45.4…\n 3 ((-73.69531 45.5855, -73.57236 45.69448, -73.72466 45.67183, -73.85771 45.57…\n 4 ((-66.32412 44.25732, -66.27378 44.29229, -66.21035 44.39204, -66.25049 44.3…\n 5 ((-68.69077 44.24873, -68.70303 44.23198, -68.70171 44.18267, -68.66118 44.1…\n 6 ((-66.89707 44.62891, -66.7625 44.68179, -66.75337 44.70981, -66.74541 44.79…\n 7 ((-68.29941 44.45649, -68.34702 44.43037, -68.40947 44.36426, -68.41172 44.2…\n 8 ((-71.39307 41.46675, -71.36533 41.48525, -71.35449 41.54229, -71.36431 41.5…\n 9 ((-74.25049 39.52939, -74.1332 39.68076, -74.10674 39.74644, -74.25317 39.55…\n10 ((-74.18818 40.6146, -74.23589 40.5187, -74.18813 40.52285, -74.13853 40.541…\n11 ((-70.67373 41.44854, -70.7605 41.37358, -70.8292 41.35898, -70.7853 41.3274…\n12 ((-71.34624 41.46938, -71.29092 41.4646, -71.24141 41.49194, -71.23203 41.65…\n13 ((-70.0627 41.32847, -70.08662 41.31758, -70.23306 41.28633, -70.05508 41.24…\n14 ((-74.9 39.14709, -74.89702 39.14546, -74.9 39.1329), (-74.9 38.95218, -74.7…\n\n\nIn this case, each record of geometry is a “MULTILINESTRING”, which is a group of one or more linestrings. Note that no other variables are in this table - it’s just the geometry.\nLet’s plot these geometries, and add the points on top.\n\nplot(coast, col = \"orange\", lwd = 2, axes = TRUE, reset = FALSE,\n     main = \"Buoys in the Gulf of Maine\")\nplot(buoys, pch = 1, cex = 0.5, add = TRUE)\ntext(buoys, labels = buoys$id, cex = 0.7, adj = c(1,-0.1))",
    "crumbs": [
      "Coding"
    ]
  },
  {
    "objectID": "C00_coding.html#array-data-aka-raster-data",
    "href": "C00_coding.html#array-data-aka-raster-data",
    "title": "Coding",
    "section": "4.4 Array data (aka raster data)",
    "text": "4.4 Array data (aka raster data)\nOften spatial data comes in grids, like regular arrays of pixels. These are great for all sorts of data like satellite images, bathymetry maps and environmental modeling data. We’ll be working with environmental modeling data which we call “Brickman data”. You can learn more about Brickman data in the wiki. We’ll be glossing over the details here, but there’s lots of detail in the wiki.\nWe’ll read in the database that tracks 82 Brickman data files, and then immediately filter out the rows that define the “PRESENT” scenario (where present means 1982–2013) and monthly climatology models.\n\ndb = brickman_database() |&gt;\n  filter(scenario == \"PRESENT\", interval == \"mon\") # note the double '==', it's comparative\ndb\n\n# A tibble: 8 × 4\n  scenario year    interval var  \n  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;\n1 PRESENT  PRESENT mon      MLD  \n2 PRESENT  PRESENT mon      Sbtm \n3 PRESENT  PRESENT mon      SSS  \n4 PRESENT  PRESENT mon      SST  \n5 PRESENT  PRESENT mon      Tbtm \n6 PRESENT  PRESENT mon      U    \n7 PRESENT  PRESENT mon      V    \n8 PRESENT  PRESENT mon      Xbtm \n\n\nIf you are wondering about filtering a table, be sure to check out the wiki on tabular data to get started.\nYou might be wondering what that |&gt; is doing. It is called a pipe, and it delivers the output of one function to the next function as the first parameter (aka argument). For example, brickman_database() produces a table, that table is immediately passed into filter() to choose rows that match our criteria.\nNow that we have the database listing just the records we want, we pass it to the read_brickman() function.\n\ncurrent = read_brickman(db)\ncurrent\n\nstars object with 3 dimensions and 8 attributes\nattribute(s):\n               Min.      1st Qu.       Median         Mean      3rd Qu.\nMLD    1.011275e+00  5.583339810 15.967359543 18.910421492 2.809953e+01\nSbtm   2.324167e+01 32.136343956 34.232215881 33.507147254 3.491243e+01\nSSS    1.644333e+01 30.735633373 31.104771614 31.492407921 3.203519e+01\nSST   -7.826599e-01  6.434107542 12.359498501 12.151707840 1.763068e+01\nTbtm  -2.676387e-01  3.595118523  6.110801697  6.122372065 7.521761e+00\nU     -2.121380e-01 -0.010892980 -0.002634738 -0.010139401 7.229637e-04\nV     -1.883337e-01 -0.010722862 -0.002858645 -0.008474233 9.565173e-04\nXbtm   3.275602e-06  0.001458065  0.003088348  0.008360344 7.256525e-03\n              Max.  NA's\nMLD   106.69815063 59796\nSbtm   35.15741730 59796\nSSS    35.59160995 59796\nSST    26.43147278 59796\nTbtm   24.60999298 59796\nU       0.07469980 59796\nV       0.05264002 59796\nXbtm    0.18996811 59796\ndimension(s):\n      from  to offset    delta refsys point      values x/y\nx        1 121 -74.93  0.08226 WGS 84 FALSE        NULL [x]\ny        1  89  46.08 -0.08226 WGS 84 FALSE        NULL [y]\nmonth    1  12     NA       NA     NA    NA Jan,...,Dec    \n\n\nThis loads quite a complex set of arrays, but they have spatial information attached in the dimensions section. The x and y dimensions represent longitude and latitude respectively. The 3rd dimension, month, is time based.\nHere we plot all 12 months of sea surface temperature, SST. Note the they all share the same color scale so that they are easy to compare.\n\nplot(current['SST'])\n\n\n\n\n\n\n\n\nJust as we are able to plot linestrings/polygons along side points, we can also plot these with arrays (rasters). To do this for one month (“Apr”) of one variable (“SSS”) we simply need to slice that data out of the current variable.\n\napril_sss = current['SSS'] |&gt;\n  slice(\"month\", \"Apr\")\napril_sss\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n         Min. 1st Qu.   Median    Mean  3rd Qu.     Max. NA's\nSSS  16.44333 30.8342 31.10334 31.4641 31.93447 35.59161 4983\ndimension(s):\n  from  to offset    delta refsys point x/y\nx    1 121 -74.93  0.08226 WGS 84 FALSE [x]\ny    1  89  46.08 -0.08226 WGS 84 FALSE [y]\n\n\nThen it’s just plot, plot, plot.\n\nplot(april_sss, axes = TRUE, reset = FALSE)\nplot(coast, add = TRUE, col = \"orange\", lwd = 2)\nplot(buoys, add = TRUE, pch = 16, col = \"purple\")\n\nWarning in plot.sf(buoys, add = TRUE, pch = 16, col = \"purple\"): ignoring all\nbut the first attribute\n\n\n\n\n\n\n\n\n\nWe can plot ALL twelve months of a variable (“SST”) with the coast and points shown. There is one slight modification to be made since a single call to plot() actually gets invoked 12 times for this data. So where do we add in the buoys and coast? Fortunately, we can create what is called a “hook” function - who knows where the name hook came from? Once the hook function is defined, it will be applied to the each of the 12 subplots.\n\n# a little function that gets called just after each sub-plot\n# it simple adds the coast and buoy\nadd_coast_and_buoys = function(){\n  plot(coast, col = \"orange\", lwd = 2, add = TRUE)\n  plot(buoys, pch = 16, col = \"purple\", add = TRUE)\n}\n\n# here we call the plot, and tell R where to call `add_coast_and_buoys()` after\n# each subplot is made\nplot(current['SST'], hook = add_coast_and_buoys)\n\nWarning in plot.sf(buoys, pch = 16, col = \"purple\", add = TRUE): ignoring all\nbut the first attribute\nWarning in plot.sf(buoys, pch = 16, col = \"purple\", add = TRUE): ignoring all\nbut the first attribute\nWarning in plot.sf(buoys, pch = 16, col = \"purple\", add = TRUE): ignoring all\nbut the first attribute\nWarning in plot.sf(buoys, pch = 16, col = \"purple\", add = TRUE): ignoring all\nbut the first attribute\nWarning in plot.sf(buoys, pch = 16, col = \"purple\", add = TRUE): ignoring all\nbut the first attribute\nWarning in plot.sf(buoys, pch = 16, col = \"purple\", add = TRUE): ignoring all\nbut the first attribute\nWarning in plot.sf(buoys, pch = 16, col = \"purple\", add = TRUE): ignoring all\nbut the first attribute\nWarning in plot.sf(buoys, pch = 16, col = \"purple\", add = TRUE): ignoring all\nbut the first attribute\nWarning in plot.sf(buoys, pch = 16, col = \"purple\", add = TRUE): ignoring all\nbut the first attribute\nWarning in plot.sf(buoys, pch = 16, col = \"purple\", add = TRUE): ignoring all\nbut the first attribute\nWarning in plot.sf(buoys, pch = 16, col = \"purple\", add = TRUE): ignoring all\nbut the first attribute\nWarning in plot.sf(buoys, pch = 16, col = \"purple\", add = TRUE): ignoring all\nbut the first attribute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoding Assignment\n\n\n\nUse the menu option File &gt; New File &gt; R Script to create a blank file. Save the file (even though it is empty) in the “assignment” directory as “assignment_script_1.R”. Use this file to build a script that meets the following challenge. Note that the existing file, “assignment_script_0.R” is already there as an example.\nUse the Brickman tutorial to extract data from the location of Buoy M01 for RCP4.5 2055. Make a plot of SST (y-axis) as a function of month (x-axis). Here’s one possible outcome.\n\n\n\nBuoy M01, RCP4.5 2055",
    "crumbs": [
      "Coding"
    ]
  },
  {
    "objectID": "C02_background.html",
    "href": "C02_background.html",
    "title": "Background",
    "section": "",
    "text": "Traditional ecological surveys are systematic, for a given species survey data sets tell us where the species is found and where it is absent. Using an observational data (like OBIS) set we only know where the species is found, which leaves us guessing about where they might not be found. This difference is what distinguishes a presence-abscence data set from a presence-only data set, and this difference guides the modeling process.\nWhen we model, we are trying to define the environs where we should expect to find a species as well as the environs we would not expect to find a species. We have in hand the locations of observations, and we can extract the environmental data at those locations. But to characterize the less suitable environments we are going to have to sample what is called “background”. We want these background samples to roughly match the regional preferences of the observations; that is we want to avoid having observations that are mostly over Georges Bank while our background samples are primarily around the Bay of Fundy.",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "C02_background.html#sample-background",
    "href": "C02_background.html#sample-background",
    "title": "Background",
    "section": "2.1 Sample background",
    "text": "2.1 Sample background\nWhen we sample the background, we are creating the input for the model if we request that the observations (presences) are joined with the background.\nNext we sample the background as guided by the density map. We’ll ask for 2x as many presences, but it is just a request. We also request that no background point be further than 30km (30000m) from it’s closest presence point.\n\ngreedy_input = sample_background(obs, mask, \n                              n = 2 * nrow(obs),\n                              class_label = \"background\",\n                              method = c(\"dist_max\", 30000),\n                              return_pres = TRUE)\n\nWarning in sample_background(obs, mask, n = 2 * nrow(obs), class_label = \"background\", : There are fewer available cells for raster 'NA' (2459 presences) than the requested 4918 background points. Only 4818 will be returned.\n\ngreedy_input\n\nSimple feature collection with 7277 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -74.89169 ymin: 38.805 xmax: -65.02004 ymax: 45.21401\nGeodetic CRS:  WGS 84\n# A tibble: 7,277 × 2\n   class                geometry\n * &lt;fct&gt;             &lt;POINT [°]&gt;\n 1 presence    (-72.8074 39.056)\n 2 presence      (-71.343 40.52)\n 3 presence  (-68.7691 41.52448)\n 4 presence       (-67.79 43.32)\n 5 presence (-68.44324 42.61177)\n 6 presence    (-72.4328 40.213)\n 7 presence   (-71.8784 40.3569)\n 8 presence      (-65.78 43.195)\n 9 presence       (-70.5 42.767)\n10 presence   (-72.3024 40.1862)\n# ℹ 7,267 more rows\n\n\nYou may encounter a warning message that says, “There are fewer available cells for raster…”. This is useful information, there simply weren’t a lot of non-NA cells to sample from. Let’s plot this.\n\nplot(greedy_input['class'], \n     axes = TRUE,  \n     pch = \".\", \n     extent = mask, \n     main = \"August greedy class distribution\",\n     reset = FALSE)\nplot(coast, col = \"orange\", add = TRUE)\n\n\n\n\n\n\n\n\nHmmm, let’s tally the class labels.\n\ncount(greedy_input, class)\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: -74.89169 ymin: 38.805 xmax: -65.02004 ymax: 45.21401\nGeodetic CRS:  WGS 84\n# A tibble: 2 × 3\n  class          n                                                      geometry\n* &lt;fct&gt;      &lt;int&gt;                                              &lt;MULTIPOINT [°]&gt;\n1 presence    2459 ((-65.07 42.68), (-65.067 42.65), (-65.05 42.583), (-65.05 4…\n2 background  4818 ((-65.02004 42.25251), (-65.02004 42.74609), (-65.1023 42.66…\n\n\nWell, that’s imbalanced with a different number presences than background points. But, on the bright side, the background points are definitely in the region of observations.",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "C02_background.html#a-function-we-can-reuse",
    "href": "C02_background.html#a-function-we-can-reuse",
    "title": "Background",
    "section": "5.1 A function we can reuse",
    "text": "5.1 A function we can reuse\nHere we make a function that needs at least three arguments: the complete set of observations, the mask used for sampling (and possibly thinning) and the month to filter the observations. The pseudo-code might look like this…\nfor a given month\n  filter the obs for that month\n  make the greedy model input by sampling the background\n    save the greedy model input\n  thin the obs\n  make the conservative model input by sampling background\n    save the conservative model input\n  return a list the greedy and conservative model inputs\nPhew! That’s a lot of steps. To manually run those steps 12 times would be tedious, so we roll that into a function that we can reuse 12 times instead.\nThis function will have a name, make_model_input_by_month. It’s a long name, but it makes it obvious what it does. First we start with the documentation.\n\n#' Builds greedy and conservative model input data sets for a given month\n#' \n#' @param mon chr the month abbreviation for the month of interest (\"Jan\" by default)\n#' @param obs table, the complete observation data set\n#' @param raster stars, the object that defines the sampling space, usually a mask\n#' @param species chr, the name of the species prepended to the name of the output files.\n#'   (By default \"Mola mola\" which gets converted to \"Mola_mola\")\n#' @param path the output data path to store this data (be default \"model_input\")\n#' @param min_obs num this sets a threshold below which we wont try to make a model. (Default is 3)\n#' @return a named two element list of greedy and conservative model inputs - they are tables\nmake_model_input_by_month  = function(mon = \"Jan\",\n                                      obs = read_observations(\"Mola mola\"),\n                                      raster = NULL,\n                                      species = \"Mola mola\",\n                                      path = data_path(\"model_input\"),\n                                      min_obs = 3){\n  # the user *must* provide a raster\n  if (is.null(raster)) stop(\"please provide a raster\")\n  # filter the obs\n  obs = obs |&gt;\n    filter(month == mon[1])\n  \n  # check that we have at least some records, if not enough then alert the user\n  # and return NULL\n  if (nrow(obs) &lt; min_obs){\n    warning(\"sorry, this month has too few records: \", mon)\n    return(NULL)\n  }\n  \n  # make sure the output path exists, if not, make it\n  make_path(path)\n  \n  \n  # make the greedy model input by sampling the background\n  greedy_input = sample_background(obs, raster,\n                                   n = 2 * nrow(obs),\n                                   class_label = \"background\",\n                                   method = c(\"dist_max\", 30000),\n                                   return_pres = TRUE)\n  # save the greedy data\n  filename = sprintf(\"%s-%s-greedy_input.gpkg\", \n                     gsub(\" \", \"_\", species),\n                     mon)\n  write_sf(greedy_input, file.path(path, filename))\n  \n  # thin the obs\n  obs = thin_by_cell(obs, raster)\n  \n  # make the conservative model\n  conservative_input = sample_background(obs, raster,\n                                   n = 2 * nrow(obs),\n                                   class_label = \"background\",\n                                   method = c(\"dist_max\", 30000),\n                                   return_pres = TRUE)\n  \n  # save the conservative data\n  filename = sprintf(\"%s-%s-conservative_input.gpkg\", \n                     gsub(\" \", \"_\", species),\n                     mon)\n  write_sf(conservative_input, file.path(path,filename))\n  \n  # make a list\n  r = list(greedy = greedy_input, conservative = conservative_input)\n  \n  # return, but disable automatic printing\n  invisible(r)\n}",
    "crumbs": [
      "Background"
    ]
  },
  {
    "objectID": "C04_modeling.html",
    "href": "C04_modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "There are no passengers on spaceship earth. We are all crew.\n\nMarshall McLuhan\nModeling starts with a collection of observations (presence and background for us!) and ends up with a collection of coeefficients that can be used with one or more formulas to make a predicition for the past, the present or the future. We are using modeling specifically to make habitat suitability maps for select species under two climate scenarios (RCP45 and RCP85) at two different times (2055 and 2075) in the future.\nModels can be used in isolation, or they can be used in combinations, called ensembles. When working with ensembles, we are chosing to create two or more models. When we use that ensemble to make a prediction, each model produces its own prediction and then we combine them (usually something like the average.)",
    "crumbs": [
      "Modeling"
    ]
  },
  {
    "objectID": "C04_modeling.html#make-a-recipe",
    "href": "C04_modeling.html#make-a-recipe",
    "title": "Modeling",
    "section": "4.1 Make a recipe",
    "text": "4.1 Make a recipe\nA recipe at a bare minimum needs to know two things: what is that data it has to work with and what is the relationship among the variables. The latter is expressed as a formula, very similar to how we specify the formula of a line with y = mx + b or a parabola y ~ ax^2 + bx + c.\n\n\n\n\n\n\nNote\n\n\n\nWe often think of formulas as left-hand side (LHS) and right-hand side (RHS) equalities. And usually, the LHS is the outcome while the RHS is about the inputs. For our modeling, the outcome is to predict the across the entire domain. We can generalize the idea with the “is a function of” operator ~ (the tilde). For yhe classic formula for a line it like this… y ~ x.\nConsider a situation where we have reduced all of the suitabke variables to Sbtm, Tbtm, MLD andXbtm, which we have in a table along with a class variable. In our case we have the outcome is an prediction of class it is a function of variables like Sbtm, Tbtm, MLD, Xbtm, etc. This formula would look like y ~ Sbtm + Tbtm + MLD + Xbtm. Unlike the specific equation for a line or parabola, we don’t pretend to know what coefficients, powers and that sort of stuff looks like. We are just saying that class is a function of all of those variables (somehow).\nIn the case where table where the outcome (class) is a function of all other variables in the table, we have a nice short hand. class ~ . where the dot means “every other variable”.\n\n\nBelow we make a recipe which is like template. We provide the table of variables (with spatial info) - techinally we need only need to provide one row as it is building a template, it doesn’t store the data (yet). We also provide the formula which we read as “class as a funtion of everything else”. The recipe() knows detects the geometry column and handles that specially.\n\naug_recipe = recipe(variables, formula = class ~ .)\naug_recipe\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 7\ncoords:    2",
    "crumbs": [
      "Modeling"
    ]
  },
  {
    "objectID": "C04_modeling.html#make-a-workflow-set",
    "href": "C04_modeling.html#make-a-workflow-set",
    "title": "Modeling",
    "section": "4.2 Make a workflow set",
    "text": "4.2 Make a workflow set\nHere’s where we specify which model(s) we want, and that we want them to be treated as an ensemble rather than as a individual models.\n\nmodels &lt;-\n  # create the workflow_set\n  workflow_set(\n    preproc = list(default = aug_recipe),\n    models = list(\n      # the standard glm specs - no tuneable parameters available\n      glm = sdm_spec_glm(),\n      # rf specs with tuning\n      rf = sdm_spec_rf(),\n      # boosted tree model (gbm) specs with tuning\n      gbm = sdm_spec_boost_tree(),\n      # maxent specs with tuning\n      maxent = sdm_spec_maxent()\n    ),\n    # make all combinations of preproc and models,\n    cross = TRUE ) |&gt;\n  # tweak controls to store information needed later to create the ensemble\n  option_add(control = control_ensemble_grid())\n\nWe have chosen these 4 models types to be memebers of our ensemble because they are common models types used for species distribution models. We aren’t limited to these, but we will use these for the tutorial.\nWhen we print the models object we see that it, too, is a table, but some of it’s columns are list types, and some have empty elements. That’s because we still have not actually modeled anything… we are still in the building phase.\n\nmodels\n\n# A workflow set/tibble: 4 × 4\n  wflow_id       info             option    result    \n  &lt;chr&gt;          &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 default_glm    &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;\n2 default_rf     &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;\n3 default_gbm    &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;\n4 default_maxent &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;",
    "crumbs": [
      "Modeling"
    ]
  },
  {
    "objectID": "C04_modeling.html#auto-tuning-the-ensemble-models",
    "href": "C04_modeling.html#auto-tuning-the-ensemble-models",
    "title": "Modeling",
    "section": "4.3 Auto-tuning the ensemble models",
    "text": "4.3 Auto-tuning the ensemble models\nNow we can allow the underlying software to run multiple times as it looks to optimize the different parameters that each model has (if the model has tuneable parameters). Here we ask for the software iterate over the crossfolds sampling set for each model in the ensemble. We then request the standard diagnostics metrics used in species distribution modeling:Boyce’s Continuous Index, the area under the Receiver Operator Curve (auc), and True Skills Statistic (tss).\nDepending upon the complexity of your set up this can take a while. We no longer are working with a template, but rather with full-blown datasets.\n\nmodels &lt;- models |&gt;\n  workflow_map(\"tune_grid\",\n    resamples = crossfolds, \n    grid = 3,\n    metrics = sdm_metric_set(), \n    verbose = TRUE)\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni 1 of 4 resampling: default_glm\n\n\n✔ 1 of 4 resampling: default_glm (609ms)\n\n\ni 2 of 4 tuning:     default_rf\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n✔ 2 of 4 tuning:     default_rf (42.3s)\n\n\ni 3 of 4 tuning:     default_gbm\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n✔ 3 of 4 tuning:     default_gbm (31.5s)\n\n\ni 4 of 4 tuning:     default_maxent\n\n\n✔ 4 of 4 tuning:     default_maxent (13.7s)\n\n\nWe get some messages as it processes, but it is hard for the novice to know what they mean, but at least they aren’t error messages. But we can print the updated models and compare to what we had before.\n\nmodels\n\n# A workflow set/tibble: 4 × 4\n  wflow_id       info             option    result   \n  &lt;chr&gt;          &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n1 default_glm    &lt;tibble [1 × 4]&gt; &lt;opts[4]&gt; &lt;rsmp[+]&gt;\n2 default_rf     &lt;tibble [1 × 4]&gt; &lt;opts[4]&gt; &lt;tune[+]&gt;\n3 default_gbm    &lt;tibble [1 × 4]&gt; &lt;opts[4]&gt; &lt;tune[+]&gt;\n4 default_maxent &lt;tibble [1 × 4]&gt; &lt;opts[4]&gt; &lt;tune[+]&gt;\n\n\nWe can see that there is more information in there than there was before. Note that the result for default_glm is labeled differently than the others; that’s because that particular model doesn’t have any tuneable parameters so the software couldn’t tune anything.",
    "crumbs": [
      "Modeling"
    ]
  },
  {
    "objectID": "C04_modeling.html#ensemble-model-metrics",
    "href": "C04_modeling.html#ensemble-model-metrics",
    "title": "Modeling",
    "section": "4.4 Ensemble model metrics",
    "text": "4.4 Ensemble model metrics\nLet’s plot the models in the ensemble, which will reveal some of the diagnostic metrics.\n\nautoplot(models)\n\n\n\n\n\n\n\n\nNotice that there are three replicates for each model except for logistic_reg which is the glm that had no tuneable parameters. Also note that the order they are represented in each plot is established by the order of the first plot and then carried through to the others. We can probably agree by examining the left hand plot, that the maxent model has the highest Boyce Continuous Index which means it’s rank is 1. Confusingly higher rank values means lower performance; maybe we should think of it a “first place”.\nBut what decides that Boyce Continuous Index should be allowed to establish the order?  It’s just alphabetical order! To be frank, the workflow rank is not all that meaningful.\nAt this point we can decide of we are satisfied with how the individual models perform with the crossfolds. If we found them lacking we could go back to add or remove models, adjust our sampling scheme, change the covariate selection or anything else we felt might improve the performance. We won’t do that now, but we’ll accept the tuned parameters “as is” and push ahead.",
    "crumbs": [
      "Modeling"
    ]
  },
  {
    "objectID": "C04_modeling.html#establish-the-ensemble",
    "href": "C04_modeling.html#establish-the-ensemble",
    "title": "Modeling",
    "section": "4.5 Establish the ensemble",
    "text": "4.5 Establish the ensemble\nNow we come to point we make a leap and build the actual ensemble - this is the thing we shall be able to use to make predictions. We only need to specify which of the metrics to preferentially use when selecting the optimized parameters. Are you ready for this?\n\nensemble &lt;- \n  simple_ensemble() |&gt;                       # make an empty ensemble\n  add_member(models, metric = \"boyce_cont\")  # populate it\n\nThat’s it - tahdah! We’re done! We can plot it.\n\nautoplot(ensemble)\n\n\n\n\n\n\n\n\nNotice that we are no longer looking at the crossfold replicates but instead we are looking at summary of the individual models run on the complete data sets. Again, “Workflow Rank” in an arbitrary order. We can see that maxent performs well on all of the metrics, but Random Forests and Boosted Regression Trees are pretty good, too. The Generalize Linear Model seems to be the odd one out, which isn’t to say that it doesn’t work.\nWe can also pull out a table of these metric results.\n\nensemble |&gt;\n  collect_metrics()\n\n# A tibble: 12 × 5\n   wflow_id       .metric     mean std_err     n\n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n 1 default_glm    boyce_cont 0.403  0.412      3\n 2 default_glm    roc_auc    0.606  0.0419     3\n 3 default_glm    tss_max    0.217  0.0345     3\n 4 default_rf     boyce_cont 0.869  0.0854     3\n 5 default_rf     roc_auc    0.663  0.0300     3\n 6 default_rf     tss_max    0.250  0.0488     3\n 7 default_gbm    boyce_cont 0.803  0.0932     3\n 8 default_gbm    roc_auc    0.680  0.0262     3\n 9 default_gbm    tss_max    0.297  0.0286     3\n10 default_maxent boyce_cont 0.790  0.171      3\n11 default_maxent roc_auc    0.692  0.0400     3\n12 default_maxent tss_max    0.336  0.0420     3\n\n\nUseful, but prettier as a plot!",
    "crumbs": [
      "Modeling"
    ]
  }
]